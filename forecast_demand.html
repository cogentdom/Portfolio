<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property=”og:title” content="Portfolio || Forecast Product Demand"/>
    <meta property=”og:type” content=”website” />
    <meta property="og:image" content="https://portingdata.com/image_files/open_graph_image.png"/>
    <meta property="og:url" content="https://portingdata.com/ipi"/>

    <meta name="description" content="Portfolio || Forecast Product Demand">

    <title>Porting Data</title>
    <link href="style.css" type="text/css" rel="stylesheet">
    <link href="icons.css" type="text/css" rel="stylesheet">
    <link href="textfield.css" type="text/css" rel="stylesheet">
    <link href="forecast_demand.css" type="text/css" rel="stylesheet">
</head>
<body>
    <!-- Header Strip -->
    <div class="header">
        <div class="title">
            <h1>Porting Data  | |  </h1><h1 id="header_title">Forecast Product Demand</h1>
        </div>
        <div></div>
        <div>
            <!-- Navigation Bar -->
            <nav class="navbar">            
                <a href="index.html">Home</a>
                <a href="ipi_report.html">Strategic Insight</a>
                <a href="btree.html">Bioinformatics</a>
            </nav>
        </div>
    </div>


    
    <section class="body">
        <!-- Icon Sidebar -->
        <div id="icons">
            <div class="row">
              <h4>Dominik Huffield</h4>
            </div> 
            <a href="https://github.com/cogentdom" target="_blank"> 
            <img class="logostyle_a" src="image_files/github_logo.png"><p>Github</p>
            </a>
            <a href="https://www.linkedin.com/in/dominik-huffield" target="_blank">
            <img class="logostyle_a" src="image_files/linkedin_logo.png"><p>LinkedIn</p>
            </a>
            <a href="https://tensoraudio.com/" target="_blank">
            <img class="logostyle_b" src="image_files/tensoraudio_logo.png"><p>Tensor Audio</p>
            </a>
            <a href="image_files/Dominik_Resume_2022.pdf" target="_blank">
            <img class="logostyle_a" src="image_files/resume_logo.png"><p>Resume</p>
            </a>
        </div>

        <div id="textfield_pfs" class="textfield">
            <!-- Financial Spending Model -->
            <h2 id="h2_small">Forecast Product Demand</h2>
            
            <section id="sec_small" class="h2_sec">
                <p>
                    Developed end to end this project offers retail stores a more efficient solution to their supply chain by identifying future sales of inventory. 
                    Allowing retailers to act preemptively and avoid risks resulting from the bull whip effect along their supply chain. 
                    Due to the effectiveness of this model the retailers utilizing this tool are able to understand future cost allowing them to accurately allocate resources day in and day out.
                </p>     
                <p>View the <a href="https://github.com/the-chiefdom/Predict_Future_Sales" target="_blank" style="color: #3366cc;">Project Repository</a> for a deeper dive into the code</p>
                
                <p>View the <a href="https://www.kaggle.com/c/competitive-data-science-predict-future-sales/data" target="_blank" style="color: #3366cc;">Kaggle dataset</a> sourcing this project</p>
                <!-- Overview and Purpose: -->
                <h3>Overview and Purpose</h3>
                <section class="h3_sec">
                    <p>
                        This Kaggle competition serves to test people's forecasting abilities by providing a challenging time-series dataset consisting of daily sales data, kindly provided by one of the largest Russian software firms - 1C Company. 
                        They are asking us to predict total sales for every product and store in the next month. 
                        By solving this competition we will be able to apply and enhance our data science skills.
                        <br>
                        To tackle this problem we will also exercise our understanding of sequencial deep learning models.
                        We will also be utilizing AWS as to maintain agility on our local machine during some of the larger computations.
                        <br><br>
                        Shown below is the pipeline for this project and will act as a roadmap for this walk through.
                    </p>
                    <h5>Project Pipeline</h5>
                    <section class="h5_sec">
                        <img class="text_img" src="image_files/predict_sales_diagram.png">

                        <!-- Pipeline Flow -->
                        <ul class="list_unsorted">
                            <h4>Pipeline Flow</h4>
                            <li style="font-weight: lighter;">Data Acqisition</li>
                                <ul id="list_sub">
                                    <li style="font-weight: lighter;">csv files</li>
                                </ul>
                            <li style="font-weight: lighter;">Data Managment</li>
                                <ul id="list_sub">
                                    <li style="font-weight: lighter;">AWS EC2 instances</li>
                                </ul>
                            <li style="font-weight: lighter;">Data Clensing</li>
                                <ul id="list_sub">
                                    <li style="font-weight: lighter;">Remove Noise</li>
                                    <li style="font-weight: lighter;">Handle Nulls</li>
                                </ul>
                            <li style="font-weight: lighter;">Preprocessing</li>
                                <ul id="list_sub">
                                    <li style="font-weight: lighter;">Stationlizing</li>
                                    <li style="font-weight: lighter;">Standardizing</li>
                                </ul>
                            <li style="font-weight: lighter;">Feature Selection</li>
                                <ul id="list_sub">
                                    <li style="font-weight: lighter;">Dimensionality Reduction</li>
                                    <li style="font-weight: lighter;">Auto Encoding</li>
                                </ul>
                            <li style="font-weight: lighter;">Modeling</li>
                                <ul id="list_sub">
                                    <li style="font-weight: lighter;">LSTM model</li>
                                    <li style="font-weight: lighter;">Cross-Validation</li>
                                    <li style="font-weight: lighter;">Hyperparameter optimization</li>
                                </ul>
                            <li style="font-weight: lighter;">Evaluation</li>
                                <ul id="list_sub">
                                    <li style="font-weight: lighter;">Unseen Data Validation</li>
                                    <li style="font-weight: lighter;">R^2</li>
                                    <li style="font-weight: lighter;">RMSE</li>
                                </ul>
                        </ul>
                        <p></p>
                    </section>
                </section>

                <!-- Data Acquisition and Management: -->
                <h3>Data Acquisition and Management</h3>
                <section class="h3_sec">
                    <p>
                        Once enrolled the data can be downloaded as a csv directly from Kaggle under the dataset tab for the competition. 
                        The CSV files provided by Kaggle are small but we will come to discover that the provided data dropped all records containing a null value; this will be addressed further in the next section. 
                        Since our data will grow significantly during preprocessing it might be preferable to run our computations on a remote server. 
                        To keep things simple we used AWS's Sagemaker to create a Jupyter notebook with an ml.t3.2xlarge kernel costing about 40 cents/hour.
                        Once the notebook instance is up and running upload the CSV files and feel free to link the instance to your Github repository.
                    </p>
                </section>
                
                <!-- Data Cleaning: -->
                <h3>Data Cleaning</h3>
                <section class="h3_sec">
                    <p>
                        Make sure to perform a thorough EDA along with utilize the dashboard Kaggle provides on the competition's data tab, from here on I will assume there is a basic understanding of the dataset and it's structure. 
                        <br><br>
                        To kick things off we want to read the sales_train.csv file into a dataframe, let's call it 'train', which will allow for some interpretable manipulations.
                        As I mentioned above Kaggle did not provide any null values within the sales_train dataset. 
                        Essentially Kaggle did not provide a record for products with a zero sales count at any given shop. 
                        These records are important to include because they have a value of zero rather than null. 
                        By including these records in our data we now have the ability to predict the demand for any given product at any given shop, even when a shop has never offered said product. 
                        <br><br>
                        To achieve this grab all unique values for shop_id and item_id, storing them in their own list variables. 
                        Now use those two variables to create a new list that is a permutation of shops and items where each element is a 2-tuple (shop_id, item_id); a list comprehension is the simplest approach. 
                        Provide this list as the data to create a new dataframe, lets call it 'index'. 
                        Next do a left merge with the index dataframe and the train dataframe (respectively).
                        Finally take the new dataframe and fill all the null values with zero then write to a csv file as to avoid redundant computations. 
                        <br><br>
                        The approach is simple enough and only takes a few lines of code. 
                        That being said as we progress and things become increasingly complex feel free to reference the repository linked above, each notebook will be associated to a section header. 
                    </p>
                </section>
                
                <!-- Preprocessing & Modeling: -->
                <h3>Preprocessing & Modeling</h3>
                <section class="h3_sec">
                    <p>
                        Starting with a new notebook read in the CSV created in the previous section. 
                        First we will need to shape the dataframe using a pivot table where shop_id and item_id are  your indecies, date_block_num is the column and aggregate item_cnt_day into item_cnt_month with a fill value of 0. 
                        The seasonality is a 3 month period for products in the aggregate so to achieve stationarity convert each sequence into it's rolling mean with a 3 month window. 
                        Next for preprocessing standardize the data using StandardScaler() then reshape the scaled data to be (214200, 33, 1) for X_train and (214200, 1) for y_train. 
                        The y_train data should be the last value available in your sequence.
                        <br><br>
                        For the model use the sequencial library in keras and add an LSTM layer with a (33, 1) input shape and a relu activation. 
                        Add three more layers first using leaky_relu then a dropout of 0.3 and last a dense layer of 1. 
                        Compile this model with a mean_squared_error loss function and an Adam optimizer. 
                        Fit the model with a 214200 batch size and 3 epochs to hit the bottom of the elbow, in the notebook I fit to 5 and saved a plot so you'll be ale to see that 3 epochs is optimal.  
                    </p>
                </section>
                
                <!-- Evaluation: -->
                <h3>Evaluation</h3>
                <section class="h3_sec">
                    <p>
                        Once the model is trained you'll want to inverse the transformation from the standardization function and then inverse the rolling mean. 
                        Evaluate the predictions of your y_hat data compared to y_actual, the competition uses root mean squared error but feel free to use whatever you want. 
                        Once you are done tuning and satisfied with your hyperparameters train on the complete data and predict for the 34th date block number.  
                        Take your predictions and clip them between 0 and 20 and save them as a CSV following the competitions desired format; submit these predictions on Kaggle. 
                        Congratulations, your have now built a deep learning model to forecast demand and will be ranked in the top half of the competition.  
                    </p>
                </section>
                
                <!-- Communicating Findings: -->
                
                <!-- Conclusion and Summary: -->
                <h3>Conclusion and Summary</h3>
                <section class="h3_sec">
                    <p>
                        Having followed the steps provided you will now be on your way with an initial design to be a real competator within this Kaggle competition. 
                        Furthermore you now have an understanding of an LSTM and its usefulness as a specialized RNN meant for forecasting sequential data. 
                        To advance your predictions even more feel free to use autoencoding potentially using tensorflows timeseriesgenerator. 
                        Another step would be to utilize some of the other files provided and do some feature engineering or better improve the layered design of your neural network. 
                        I believe this project has some real world uses esspecially in the reahlm of supply chain so I am glad to have opened this project and I hope you found it useful as a guide.
                    </p>
                </section>
                
                
            </section>   
            <p></p>
        </div> 
        
    </section>

    <footer>
        <p>&copy; 2020 Dominik Huffield</p>
    </footer>
</body>
</html>