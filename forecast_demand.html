<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property=”og:title” content="Portfolio || Forecast Product Demand"/>
    <meta property=”og:type” content=”website” />
    <meta property="og:image" content="https://portingdata.com/image_files/open_graph_image.png"/>
    <meta property="og:url" content="https://portingdata.com/ipi"/>

    <meta name="description" content="Portfolio || Forecast Product Demand">

    <title>Porting Data</title>
    <link href="style.css" type="text/css" rel="stylesheet">
    <link href="icons.css" type="text/css" rel="stylesheet">
    <link href="textfield.css" type="text/css" rel="stylesheet">
    <link href="forecast_demand.css" type="text/css" rel="stylesheet">
</head>
<body>
    <!-- Header Strip -->
    <div class="header">
        <div class="title">
            <h1>Porting Data  | |  </h1><h1 id="header_title">Forecast Product Demand</h1>
        </div>
        <div></div>
        <div>
            <!-- Navigation Bar -->
            <nav class="navbar">            
                <a href="index.html">Home</a>
                <a href="ipi_report.html">Strategic Insight</a>
                <a href="btree.html">Bioinformatics</a>
            </nav>
        </div>
    </div>


    
    <section class="body">
        <!-- Icon Sidebar -->
        <div id="icons">
            <div class="row">
              <h4>Dominik Huffield</h4>
            </div> 
            <a href="https://github.com/Chief-dom" target="_blank"> 
            <img class="logostyle_a" src="image_files/github_logo.png"><p>Github</p>
            </a>
            <a href="https://www.linkedin.com/in/dominik-huffield" target="_blank">
            <img class="logostyle_a" src="image_files/linkedin_logo.png"><p>LinkedIn</p>
            </a>
            <a href="https://tensoraudio.com/" target="_blank">
            <img class="logostyle_b" src="image_files/tensoraudio_logo.png"><p>Tensor Audio</p>
            </a>
            <a href="image_files/Dominik_Resume_2021.pdf" target="_blank">
            <img class="logostyle_a" src="image_files/resume_logo.png"><p>Resume</p>
            </a>
        </div>

        <div id="textfield_pfs" class="textfield">
            <!-- Financial Spending Model -->
            <h2 id="h2_small">Forecast Product Demand</h2>
            
            <section id="sec_small" class="h2_sec">
                <p>
                    Developed end to end this project offers retail stores a more efficient solution to their supply chain by identifying future sales of inventory. 
                    Allowing retailers to act preemptively and avoid risks resulting from the bull whip effect along their supply chain. 
                    Due to the effectiveness of this model the retailers utilizing this tool are able to understand future cost allowing them to accurately allocate resources day in and day out.
                </p>     
                <p>View the <a href="https://github.com/the-chiefdom/Predict_Future_Sales" target="_blank" style="color: #3366cc;">Project Repository</a> for a deeper dive into the code</p>
                
                <p>View the <a href="https://www.kaggle.com/c/competitive-data-science-predict-future-sales/data" target="_blank" style="color: #3366cc;">Kaggle dataset</a> sourcing this project</p>
                <!-- Overview and Purpose: -->
                <h3>Overview and Purpose</h3>
                <section class="h3_sec">
                    <p>
                        This Kaggle competition serves to test people's forecasting abilities by providing a challenging time-series dataset consisting of daily sales data, kindly provided by one of the largest Russian software firms - 1C Company. 
                        They are asking us to predict total sales for every product and store in the next month. 
                        By solving this competition we will be able to apply and enhance our data science skills.
                        <br>
                        To tackle this problem we will also exercise our understanding of sequencial deep learning models.
                        We will also be utilizing AWS as to maintain agility on our local machine during some of the larger computations.
                        <br><br>
                        Shown below is the pipeline for this project and will act as a roadmap for this walk through.
                    </p>
                    <h5>Project Pipeline</h5>
                    <section class="h5_sec">
                        <img class="text_img" src="image_files/predict_sales_diagram.png">

                        <!-- Pipeline Flow -->
                        <ul class="list_unsorted">
                            <h4>Pipeline Flow</h4>
                            <li style="font-weight: lighter;">Data Acqisition</li>
                                <ul id="list_sub">
                                    <li style="font-weight: lighter;">csv files</li>
                                </ul>
                            <li style="font-weight: lighter;">Data Managment</li>
                                <ul id="list_sub">
                                    <li style="font-weight: lighter;">AWS EC2 instances</li>
                                </ul>
                            <li style="font-weight: lighter;">Data Clensing</li>
                                <ul id="list_sub">
                                    <li style="font-weight: lighter;">Remove Noise</li>
                                    <li style="font-weight: lighter;">Handle Nulls</li>
                                </ul>
                            <li style="font-weight: lighter;">Preprocessing</li>
                                <ul id="list_sub">
                                    <li style="font-weight: lighter;">Stationlizing</li>
                                    <li style="font-weight: lighter;">Standardizing</li>
                                </ul>
                            <li style="font-weight: lighter;">Feature Selection</li>
                                <ul id="list_sub">
                                    <li style="font-weight: lighter;">Dimensionality Reduction</li>
                                    <li style="font-weight: lighter;">Auto Encoding</li>
                                </ul>
                            <li style="font-weight: lighter;">Modeling</li>
                                <ul id="list_sub">
                                    <li style="font-weight: lighter;">LSTM model</li>
                                    <li style="font-weight: lighter;">Cross-Validation</li>
                                    <li style="font-weight: lighter;">Hyperparameter optimization</li>
                                </ul>
                            <li style="font-weight: lighter;">Evaluation</li>
                                <ul id="list_sub">
                                    <li style="font-weight: lighter;">Unseen Data Validation</li>
                                    <li style="font-weight: lighter;">R^2</li>
                                    <li style="font-weight: lighter;">RMSE</li>
                                </ul>
                        </ul>
                        <p></p>
                    </section>
                </section>

                <!-- Data Acquisition and Management: -->
                <h3>Data Acquisition and Management</h3>
                <section class="h3_sec">
                    <p>
                        Once enrolled the data can be downloaded as a csv directly from Kaggle under the dataset tab for the competition. 
                        The CSV files provided by Kaggle are small but we will come to discover that the provided data dropped all records containing a null value; this will be addressed further in the next section. 
                        Since our data will grow significantly during preprocessing it might be preferable to run our computations on a remote server. 
                        To keep things simple we used AWS's Sagemaker to create a Jupyter notebook with an ml.t3.2xlarge kernel costing about 40 cents/hour.
                        Once the notebook instance is up and running upload the CSV files and feel free to link the instance to your Github repository.
                    </p>
                </section>
                
                <!-- Data Cleaning: -->
                <h3>Data Cleaning</h3>
                <section class="h3_sec">
                    <p>
                        
                    </p>
                </section>
                
                <!-- Exploratory Data Analysis: -->
                <h3>Preprocessing</h3>
                <section class="h3_sec">
                    <p></p>
                </section>
                
                <!-- Feature Selection: -->
                <h3>Feature Selection</h3>
                <section class="h3_sec">
                    <p></p>
                </section>
                
                <!-- Modeling: -->
                <h3>Modeling</h3>
                <section class="h3_sec">
                    <p></p>
                </section>
                
                <!-- Evaluation: -->
                <h3>Evaluation</h3>
                <section class="h3_sec">
                    <p></p>
                </section>
                
                <!-- Communicating Findings: -->
                
                <!-- Conclusion and Summary: -->
                <h3>Conclusion and Summary</h3>
                <section class="h3_sec">
                    <p></p>
                </section>
                
                
            </section>   
            <p></p>
        </div> 
        
    </section>

    <footer>
        <p>&copy; 2020 Dominik Huffield</p>
    </footer>
</body>
</html>