<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property=”og:title” content="Portfolio || Idaho Policy Institute Report"/>
    <meta property=”og:type” content=”website” />
    <meta property="og:image" content="https://portingdata.com/image_files/open_graph_image.png"/>
    <meta property="og:url" content="https://portingdata.com/ipi"/>

    <meta name="description" content="Portfolio || Idaho Policy Institute Report">

    <title>Porting Data</title>
    <link href="style.css" type="text/css" rel="stylesheet">
    <link href="icons.css" type="text/css" rel="stylesheet">
    <link href="textfield.css" type="text/css" rel="stylesheet">
    <link href="ipi_report.css" type="text/css" rel="stylesheet">
</head>
<body>
    <!-- Header Strip -->
    <div class="header">
        <div class="title">
            <h1>Porting Data  | |  </h1><h1 id="header_title">Idaho Policy Institute Report</h1>
        </div>
        <div></div>
        <div>
            <!-- Navigation Bar -->
            <nav class="navbar">            
                <a href="index.html">Home</a>
                <a href="predict_sales.html">Predict Sales</a>
                <a href="btree.html">Bioinformatics</a>
            </nav>
        </div>
    </div>


    
    <section class="body">
        <!-- Icon Sidebar -->
        <div id="icons">
            <div class="row">
              <h4>Dominik Huffield</h4>
            </div> 
            <a href="https://github.com/the-chiefdom" target="_blank"> 
            <img class="logostyle_a" src="image_files/github_logo.png"><p>Github</p>
            </a>
            <a href="https://www.linkedin.com/in/dominik-huffield" target="_blank">
            <img class="logostyle_a" src="image_files/linkedin_logo.png"><p>LinkedIn</p>
            </a>
            <a href="https://github.com/the-chiefdom/Acoustics_FTB" target="_blank">
            <img class="logostyle_b" src="image_files/tensoraudio_logo.png"><p>Tensor Audio</p>
            </a>
            <a href="image_files/Dominik_Resume_2020.pdf" target="_blank">
            <img class="logostyle_a" src="image_files/resume_logo.png"><p>Resume</p>
            </a>
        </div>

        <div id="textfield_ipi" class="textfield">
            <!-- Financial Spending Model -->
            <h2 id="h2_small">Financial Spending Model</h2>
            
            <section id="sec_small" class="h2_sec">
                <p>A three month service learning project for the Idaho Policy Institute(IPI); 
                    this is a project I completed for my graduate data science course at Boise State. 
                    This Project successfully makes use of Idaho's municipal government financial data by providing insightful models to augment the decision making process. 
                    This specifically correlates government expenditure and crime allowing for the optimization of all future investments. </p>     
                <p>View the <a href="https://github.com/Chief-Dom/Graduate-Data-Science-Final" target="_blank" style="color: #3366cc;">Project Repository</a> for a deeper dive into the code</p>
                
                <!-- Meeting the Client -->
                <h3>Meeting the Client:</h3>
                <section class="h3_sec">
                    <p>Dr. Cheong Kim from the Idaho Policy Institute has assembled a file containing data on finances, population, and crime for virtually every city in Idaho. Dr. Kim and his colleagues are social scientists performing research to assist decision makers in answering the difficult decisions of the day, and helps state and local government leaders navigate change and forge strong directions for a better Idaho.  </p>
                    <h5>Objective of the Project</h5>
                    <section class="h5_sec">
                        <p>With the desire to augment the decision making process for state and local government bodies, Dr. Kim sought to improve the investment process of government funds. It was asked that my team and I identify expenditure trends relation to various crime statistics. In doing so these decison makers would have a more optimized ability to allocate resources. </p>
                    </section>
                </section>
                <!-- Initial Database -->
                <h3>Initial Database</h3>
                <section class="h3_sec">
                    <!-- Database Overview -->
                    <h5>Database Overview</h5>
                    <section class="h5_sec">
                        <!-- Idaho Municipal Databes -->
                        <ul class="list_unsorted">
                            <h4>Idaho Municipal Database</h4>
                            <li style="font-weight: lighter;">Scope: municipalities in Idaho, 1995 - 2014/5</li>
                            <li style="font-weight: lighter;">23 non-fiscal variables (e.g., government employees, crimes,
                                Metro/Micro SA status, population, pop density, nonprofits, and
                                businesses)</li>
                            <li style="font-weight: lighter;">576 fiscal variables, which derived from the Government Finance
                                Database produced by Pierson, Hand, and Thompson (2015)</li>
                        </ul>
                        <img class="text_img" src="image_files/city_a.png">
                        <img class="text_img" src="image_files/city_b.png">
                        <img class="text_img" src="image_files/city_c.png">
                    
                        <!-- Idaho County Database -->
                        <ul class="list_unsorted">
                            <h4>Idaho County Database</h4>
                            <li style="font-weight: lighter;">Scope: counties in Idaho, 1995-2015/6</li>
                            <li style="font-weight: lighter;">Currently, 11 variables (e.g., crimes, nonprofits, businesses, and
                                    income) + populations and pop density are being added by Jeff’s
                                    class.</li>
                            <li style="font-weight: lighter;">A large number of fiscal variables will be incorporated from the
                                    Government Finance Database produced by Pierson, Hand, and
                                    Thompson (2015) down the road.</li>
                        </ul>
                        <img class="text_img" src="image_files/county_a.png">
                        <img class="text_img" src="image_files/county_b.png">

                        <!-- Government Fimnance Database -->
                        <ul class="list_unsorted">
                            <h4>Government Finance Database</h4>
                            <li style="font-weight: lighter;">Idaho Municipal/County Databases: Fiscal variables for Idaho derived from
                                    the Government Finance Database produced by Pierson, Hand, and
                                    Thompson (2015) who compiled the U.S. Census Bureau’s government
                                    financial statistics.</li>
                            <li style="font-weight: lighter;">The Census Bureau’s government financial statistics are created by merging
                                    two series of financial information: Census of Government Finance and
                                    Employment Data (years ending 2 and 7) and Annual Survey of State &
                                    Local Government Finances (in-between years).</li>
                            <li style="font-weight: lighter;">Dollar amounts are expressed in thousands of nominal dollars (Pierson et
                                    al. 2015, p.3).</li>
                            <li style="font-weight: lighter;">This <a href="http://www.willamette.edu/mba/research_impact/public_datasets/" target="_blank" style="color: #3366cc;">database</a> is publicly available</li>

                            <ul id="list_sub">
                                <h4 id="list_sub_title">Advantages:</h4>
                                <li style="font-weight: lighter;">Comprehensive database in one standardized format in one file</li>
                                <li style="font-weight: lighter;">Basic but critical measures are provided, which were unavailable in
                                    the original Census Bureau’s government financial statistics; e.g.,
                                    total revenue, total expenditure among others.</li>

                                <h4 id="list_sub_title">Limitations:</h4>
                                <li style="font-weight: lighter;">Self-reported numbers: for best accuracy, actual government budget
                                    doc/CAFR should be the best. So, if you are interested in a small number of
                                    governments (say up to 10) or current budget numbers, go to the
                                    government websites and obtain the actual dollar numbers. But, if you are
                                    interested in comparing many governments (say 100) at the same time and
                                    creating an average trend chart from them, collecting data would be
                                    time-consuming. In this case, this database could be a good source.</li>
                                <li style="font-weight: lighter;">Overrepresentation of large municipalities in-between years.</li>
                                <li style="font-weight: lighter;">Lots of missings values.</li>
                            </ul>
                        </ul>
                    </section>
                    <!-- The Data Itself and Early Stage Preparation -->
                    <h5>Data Manual</h5>
                    <section class="h5_sec">
                        <p>The data was stored and presented to us as csv files. There were 201 different csv files each of which described a city in Idaho and were structured like so:</p>
                        <ul class="list_unsorted">
                            <h4>CSV Files</h4>
                            <li style="font-weight: lighter;">Spanning 20 years (unbalanced)</li>
                            <li style="font-weight: lighter;">618 columns describing finance, crime, population etc.</li>
                            <li style="font-weight: lighter;">Financial data in nominal dollars</li>
                            <li style="font-weight: lighter;">Unique values for county & city data</li>
                        </ul>
                        <p>Each of these 618 varaibales were encoded and required a manual to understand what was being represented. I am unfortunatly not at liberty to present this manual at this time however, I will fully describe all variables used in this documentation.</p>
                    </section>

                    <!-- Importing Scrapped Data -->
                    <h5>Importing new Data</h5>
                    <section class="h5_sec">
                        <p>The provided dataset contained a lot of missing values so we felt it important to add more features that possessed every data point. This was tricky because not only did we have to load data but we had to do it to every csv. To solve this we made a backend funtion that combined all of the csv files into one. After importing we merged these tables with the one provided by IPI, or in one case made an additional table. Listed below are all additional sources.</p>
                        <ul class="list_unsorted">
                            <h4>Data Sources</h4>
                            <li>U.S. Census Bureau (USC)</li>
                            <p id="list_item">Provides population measurments at various entity levels including state, county and city</p>
                            <li>Willamette Government Finance Database (GFD)</li>
                            <p id="list_item">Provides goverment cash flow information for each city/county in Idaho</p>
                            <li>Federal Bureau of Investigation (FBI) </li>
                            <p id="list_item">Describes various crime variables</p>
                            <li>Bureau of Labor Statistics (BLS) </li>
                            <p id="list_item">Allows for adjustment for inflation and provides employment related statistics</p>
                        </ul>
                        <p></p>
                    </section>
                </section>

                <!-- Backend Functions -->
                <h3>Backend Functions We Created</h3>
                <section class="h3_sec">
                    <p>
                        We created many functions and methods to simplfy our work and all future work. 
                        One of these functions converts from nominal to real dollars. 
                        There are many plotting functions such as heat maps, matrices of distributions and scatter plots, choropleth maps , etc. 
                        These methods and functions are all stored within the support folder and have their own documentation. 
                    </p>
                    <p>
                        For a deeper dive into the code view the entire <a href="https://github.com/Chief-Dom/Graduate-Data-Science-Final/tree/master/support" target="_blank" style="color: #3366cc;">support folder</a>, 
                        or specifically view one of folders three files listed below.
                    </p>
                    <ul class="list_unsorted">
                        <h4>Contents of Support Folder</h4>

                        <li><a href="https://github.com/Chief-Dom/Graduate-Data-Science-Final/blob/master/support/load_data.py" target="_blank" style="color: #3366cc;">Load Data</a></li>
                        <p id="list_item">Provides functions meant to quickly pull slices of the csv. Acting as a query the functions within this file can quickly return slices such as gps or employment data.</p>

                        <li><a href="https://github.com/Chief-Dom/Graduate-Data-Science-Final/blob/master/support/plotting_funcs.py" target="_blank" style="color: #3366cc;">Plotting & Graphing</a></li>
                        <p id="list_item">During our exploratory analysis we created functions for plots (e.g. histograms, heatmaps, choropleths) so that they may be reused. </p>

                        <li><a href="https://github.com/Chief-Dom/Graduate-Data-Science-Final/blob/master/support/supporting_funcs.py" target="_blank" style="color: #3366cc;">Utilities</a></li>
                        <p id="list_item">A vast range of functions meant for pre-processing, this file has capabilities such as normalization and converting monetary values from nominal to real dollars.</p>
                    </ul>
                </section>


                <!-- Data Exploration -->
                <h3>Data Exploratory & Cleansing:</h3>
                <section class="h3_sec">
                    <p>
                        Once we had a base understanding of the database it was time to get hands on with the data. 
                        Quickly it became apparent that most of the described values were missing.
                        With concerns of removing valuable insights, there were enough missing values that dropping rows, columns nor setting thresholds provided solutions.
                    </p>
                    <p>
                        We then began to sort our variables trying to find columns with the least missing values.
                        Using the sorted list we identfied some values we believed to be essential in describing the required objective.
                        Using the essential fields containing few missing values, we began to look for a pattern. 
                    </p>  
                    <p>
                        Since we are dealing with data having time series attributes, we need balance in our data from year to year. 
                        Below is a plot showing the quantity of populated values over each year.
                    </p>

                    <img class="text_img" src="./image_files/data_densities.png">

                    <p>
                        Shown from this graph it is clear that our data has consistency with 5 year intervals. 
                        This specific plot only represents population but we found this pattern to be consistent across most variables.
                    </p>
                    <h5>Dimensionality Reduction</h5>  
                    <section class="h5_sec"> 
                        <p>    
                            Using this information we will drop data for every year except for 1997, 2002, 2007 & 2012. 
                        </p>
                        <p>
                            Other than the time series we also had an axis for cities/counties. 
                            There were many cities with very little data so we concluded that those cities should be removed. 
                            To do so we dropped any city that did not contain data for the four years we intended to use in our model.
                            Doing so put our set at 59 of the original 200 cities. 
                            This reduction was justified because the cities that were dropped contained such little data there was likely no useful insight.
                            Similiarly counties had about 10% the amount of data as cities so we ultimatley chose to only pursue cities.
                        </p>
                        <p>
                            We have greatly improved the usefulness of our data however we still have many variables missing large amounts of data. 
                            To maintain further integrity we chose to drop all fields that do not have a populous of at least 87%.
                            This ratio was chosen to mitigate interpolation while maintaining most fields. 
                        </p>
                    </section> 
                </section>

                <!-- Finishing Preprocessing for Interpolation -->
                <h3> Finishing Preprocessing</h3>
                <section class="h3_sec">
                    <h5>Train Validation Test Split</h5>
                    <section class="h5_sec">
                        <p>
                            First Let us split the data using 25% for our test set and the remaining for training.
                            Since we do not want to perform interpolation on our test set we will drop all null records and set this set aside.
                        </p>
                    </section>
                    <h5>Interpolation</h5>
                    <section class="h5_sec">
                        <p>
                            Finally we have reached the final data set that we intend to input into our models.
                            This input data is increadibly important and is the driving force of our models.
                            Knowing the usefulness of every data point we will perform a simple linear interpolation on our points to predict the missing values with a relatively low varrience. 
                        </p>
                    </section>
                </section>

                <!-- Model Objectives -->
                <h3>Objectives for our Models</h3>
                <section class="h3_sec">
                    <p>
                        The amount of columns contained within the set describing crime amounted to 7 however there were over 500 variables describing financial revenue or expenditure. 
                        Using this knowledge we thought it best to attempt to describe crime using financial information as our independent variables (along with additional misc. data). 
                        We chose to do this so that we would not limit the features available for our models along with the ability of this type of model to provide a good description of the relation between government finances and crime. 
                        The variables describing crime were quantities of occurances so to use this as our dependent variable we needed to construct a regression model.
                    </p>
                    <h5>Selecting the OLS Time Fixed Effects Model</h5>
                    <section class="h5_sec">
                        <p>
                            Since this a panel dataset we wanted to account for the serial correlation resulting from the time series. 
                            Additionally we thought a linear model the easiest to interpret and understand the inner workings of the data itself. 
                            This lead us to a time effects model for our initial build, 
                            however the world is not linear and features will likely contain heteroskedasticity so we will need to test for this using the Breusch-Pagan Test.
                            If the tests yield results suggesting a linear model to be biased we will move forward with a non-linear approach.
                        </p>
                        <p>
                            After evaluating many models I concluded that an OLS Time Fixed Effects model was the best approach. 
                            This was done because it would account for serial correlation and provide a regression summary without additional code.
                            Additionally this could be used as a base model with a short build time, allowing for better allocation of time spent developing.
                        </p>
                    </section>
                </section>

                <!-- OLS Assumtions -->
                <h3>What is OLS and it's Assumptions</h3>
                <section class="h3_sec">
                    <p>
                        The OLS approach to regression is unbiased therefor making it reliable for analysis, however our data has to meet some conditions refered to as assumptions.  
                    </p>
                    <ul class="list_unsorted">
                        <h4>Least Squares Assumptions</h4>
                        <li>Error has a Conditional Mean Zero</li>
                            <p id="list_item">Provides population measurments at various entity levels including state, county and city</p>
                        <li>Features are identically independently distributed</li>
                            <p id="list_item">Provides goverment cash flow information for each city/county in Idaho</p>
                        <li>No Perfect MultiCollinearity</li>
                            <p id="list_item">Describes various crime variables</p>
                        <li>Large Outliers are Unlikely</li>
                            <p id="list_item">Nonzero finite fourth moments</p>
                    </ul>
                </section>

                <!-- Testing Data to meet Assumptions -->
                    <!-- Outliers -->
                    <!-- Homoskedastic -->


                <!-- Validating that OLS is BLUE -->
                <h3>Validating that OLS is BLUE</h3>
                <section class="h3_sec">
                    <p>
                        Refering to the OLS assumptions listed above we need to test our data to make sure all of the assumptions are true.
                        If the assumptions are true about our data then it is said Ordinary Least Squares (OLS) is the Best Linear Unbiased Estimate (BLUE).
                        If these assumptions are not true about our data then this approach will be biased and we will nedd to use a different approach. 
                    </p>

                    <!-- Hyperparameter -->
                    <h5>First a quick Hyperparameter</h5>
                    <section class="h5_sec">
                        <p>
                            In the next step we wish to test for independance accross our features utilizing the SelectKBest function within Scikit-Learn's library. 
                            To do this we must tell the function a value for K, representing the amount of features desired.
                            Putting a random number is not overly effective so we will attempt to optimize K by using a Lasso regression.
                            This regression works by reducing the coefficient of features to zero if they are not usful to the model.
                            Using this we will simply set K to equal the number of regressors from the Lasso.
                        </p>
                    </section>

                    <!-- Chi Squared Test -->
                    <h5>Chi Squared Test</h5>
                    <section class="h5_sec">
                        <p>
                            Having solved the hyperparameter K we then ran a Chi Squared test. 
                            The purpose of this test is to validate that our features are independant of eachother.
                            SelectKBest will performed a Chi Squared test to identify feature independance.
                            Following this the function will select the K best features for our dependant variable.
                        </p>
                    </section>

                    <!-- Handling MultiCollinearity -->
                    <h5>Handling MultiCollinearity</h5>
                    <section class="h5_sec">
                        <p>
                            A few of these features are linear combinations of one another so we will remove these and in doing so will remove the multicollinearity.
                            In addition we have some linear combinations within our records so we will fix those as well. 
                            Finally our data has passed our assumptions and we are ready to build our model. 
                        </p>
                    </section>
                </section>


                <!-- Analysis of Regression -->
                <h3>Time Fixed Effects Regression Model</h3>
                <section class="h3_sec">
                    <p>
                        Having selected "Total Theft" as our dependant variable we felt it the best to generally represent crime.
                        The data for the model describes that of 59 cities spanning 20 years (divided into 4 intervals, every 5 year).
                        Additionally there were 188 records for each of the seven features.
                    </p>
                    <h5>Summarizing Results</h5>
                    <section class="h5_sec">
                        
                        <img class="text_img" src="image_files/regress_summary.png">
                        <p>
                            From here this model appears to be a good representation of our data.
                            Using this summary our P-value is 0.0000 which is great for now, but later we will need to re-evaluate this number with unseen data.
                            In addition to being statistically significant we also have an R Squared value of 0.9019, so we should be able to say that the model is a good description of the data.
                        </p>
                        <!-- Run an F-Test using F distro. -->
                        <!-- Evaluate log likelihood -->

                    </section>
                
                    <h5>Identifying the Correlation</h5>
                    <section class="h5_sec">
                
                        <img class="text_img" src="image_files/regress_correlation.png">
                        
                        <!-- Evaluate F-test Results -->
                        <p>
                            This model appears to be a good representation of the data, however our first 6 regressors are all similiar and provide little insight.
                            The reason for this limited insight is due to the fact that as cities grow in size they will have more revenue and therfor expenditure.
                            Crime is highly correlated to population renduring little insight from these variables, however it does show that our model is working logically. 
                        </p>
                        <!-- The Signal -->
                        <h4>The Signal within the Noise</h4>
                        
                            <p>
                                The real insight come from the regressor Total_LTD_Out (Total Long Term Outstanding Debt). 
                                A closer look at this regressor shows that it is the most statistically significant with a P-value of 0.0000 and has the lowest varrience with a standard error of 0.0025.
                                With the ultimate objective of efficiently reducing crime this regressor shows promise towards an effective strategy to do so.
                                By validating the correlation between long term debt and total theft for cities in Idaho, decision makers are now able to better invest resources when combatting theft.  
                            </p>

                    </section>
                </section>

                <!-- Identify Davids (linkedin) 4 features for having made an impact-->
                
                <!-- Conclusions -->
                <h3>Conclusion</h3>
                <section class="h3_sec">

                    <p>
                        I was provided a database comprised of financial expenditure/revenue and crime for various cities in Idaho. 
                        My client, the Idaho Policy Institute, desired to know how better to allocate resources to better reduce crime. 
                        Throughout my analysis I found the data to contain missing values, was unbalance along the temporal axis and was overall sparse. 
                        However, after some data scrapping, dimensionality reduction and minimal interpolation; I was successful in cleaning the data having greatly reduced the noise. 
                    </p>
                    <p>    
                        I then used a non biased OLS time fixed effects regression model to further understand the data. 
                        Having made a statistically significant and reliable model I was able to draw a clear insight. 
                        Many of the features were that of general/overall expenditure and revenue, thus were not only largely correlated to crime but also population. 
                        This is difficult because logically, more people means more total crime even if the rates are identical. 
                    </p>
                    <p>  
                        Taking another look at the model I then noticed that the most statistically significant feature (by a large margin) was that of "long term outstanding debt." 
                        The correlation between long term debt of a city and the associated crime provided invaluable insight towards the client's desired objective, reducing crime. 
                        Using this information government bodies are better able to allocate resources; by removing the "pigeon superstition" associated to investing funds, my analysis saved local governments thousands of dollars.
                    </p>

                </section>
            </section>   
            <p></p>
        </div> 
        
    </section>

    <footer>
        <p>&copy; 2020 Dominik Huffield</p>
    </footer>
</body>
</html>